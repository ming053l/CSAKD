{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad9de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "FLOPs = 143.245606912G\n",
      "Params = 7.426272M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "FLOPs = 940.241010688G\n",
      "Params = 26.784012M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "# from sklearn.metrics import log_loss\n",
    "import random as rn\n",
    "# from models.dcsn3d import DCSNMain as DCSN\n",
    "from models.dcsn2d_light import Teacher , Student\n",
    "# from models.dcsn2d_light_MSI_awgn import DCSN_Teacher , DCSN_Student\n",
    "from utils import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from scipy.io import savemat, loadmat\n",
    "from trainOps import *\n",
    "import tqdm\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# torch.backends.cudnn.benchmark=True\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "device = 'cuda'\n",
    "MAX_EP = 16000\n",
    "BANDS = 172\n",
    "SIGMA = 0.0    ## Noise free -> SIGMA = 0.0\n",
    "\n",
    "prefix='DCSN_joint2'\n",
    "DEBUG=False\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train Convex-Optimization-Aware SR net')\n",
    "    \n",
    "    parser.add_argument('--SEED', type=int, default=1029)\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "\n",
    "    parser.add_argument('--resume_ind', type=int, default=999)\n",
    "    parser.add_argument('--snr', type=int, default=0)\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--workers', type=int, default=4)\n",
    "\n",
    "    ## Data generator configuration\n",
    "    parser.add_argument('--crop_size', type=int, default=256)\n",
    "    parser.add_argument('--scale', type=int, default=4)\n",
    "    parser.add_argument('--bands', type=int, default=172)\n",
    "    parser.add_argument('--msi_bands', type=int, default=4)\n",
    "    parser.add_argument('--mis_pix', type=int, default=0)\n",
    "    parser.add_argument('--mixed_align_opt', type=int, default=0)\n",
    "    parser.add_argument('--student_layers', type=list, default=[2,2,2,2])\n",
    "    \n",
    "    # Network architecture configuration\n",
    "    parser.add_argument(\"--network_mode\", type=int, default=1, help=\"Training network mode: 0) Single mode, 1) LRHSI+HRMSI, 2) COCNN (LRHSI+HRMSI+CO), Default: 2\")     \n",
    "    parser.add_argument('--num_base_chs', type=int, default=86, help='The number of the channels of the base feature')\n",
    "    parser.add_argument('--num_blocks', type=int, default=6, help='The number of the repeated blocks in backbone')\n",
    "    parser.add_argument('--num_agg_feat', type=int, default=172//4, help='the additive feature maps in the block')\n",
    "    parser.add_argument('--groups', type=int, default=1, help=\"light version the group value can be >1, groups=1 for full COCNN version, groups=4 is COCNN-Light for 4 HRMSI version\")\n",
    "    \n",
    "    # Others\n",
    "    # parser.add_argument(\"--root\", type=str, default=\"/work/u1657859/DCSNv2/Data\", help='data root folder')\n",
    "    parser.add_argument(\"--root\", type=str, default=\"/work/u5088463/Data\", help='data root folder')   \n",
    "    parser.add_argument(\"--test_file\", type=str, default=\"./test.txt\")   \n",
    "    parser.add_argument(\"--prefix\", type=str, default=\"KD_QRCODE_band4\")  \n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\", help=\"cuda:device_id or cpu\")  \n",
    "    parser.add_argument(\"--DEBUG\", type=bool, default=False)  \n",
    "    parser.add_argument(\"--gpus\", type=int, default=1)\n",
    "    parser.add_argument(\"-f\", type=str, default=1)\n",
    "    # parser.add_argument(\"--test_mode\", type=int, default=2 , help='1 : test teacher , 2 : test student' ) \n",
    "    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "m1=Student(args)\n",
    "m2=Teacher(args)\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "input1 = torch.randn(1, 4, 256, 256) \n",
    "input2 = torch.randn(1, 172, 64,64)           \n",
    "flops, params = profile(m1, inputs=(input2,input1, ))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')\n",
    "\n",
    "flops, params = profile(m2, inputs=(input2,input1, ))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb24b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pixelshuffle.PixelShuffle'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "FLOPs = 175.565520704G\n",
      "Params = 16.335277M\n"
     ]
    }
   ],
   "source": [
    "from PeerMethod.MSSJFL import Net\n",
    "\n",
    "m3 = Net(172,4,4)\n",
    "flops, params = profile(m3, inputs=(input2,input1, ))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c87af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "      - Flops:  134.07 GMac\n",
      "      - Params: 6.09 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "FLOPs = 133.898144704G\n",
      "Params = 4.790482M\n",
      "GPU warm up ...\n",
      "\n",
      "Inference Time Analysis ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:29<00:00, 34.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total=28020.46976661682 ms\n",
      "\n",
      "Avg=27.36374000646174 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "# from sklearn.metrics import log_loss\n",
    "import random as rn\n",
    "# from models.dcsn3d import DCSNMain as DCSN\n",
    "from models.KQD import Teacher , Student\n",
    "# from models.dcsn2d_light_MSI_awgn import DCSN_Teacher , DCSN_Student\n",
    "# from utils import *\n",
    "import torch\n",
    "import numpy as np\n",
    "# from dataset import *\n",
    "from scipy.io import savemat, loadmat\n",
    "# from trainOps import *\n",
    "import tqdm\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# torch.backends.cudnn.benchmark=True\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "device = 'cuda'\n",
    "MAX_EP = 16000\n",
    "BANDS = 172\n",
    "SIGMA = 0.0    ## Noise free -> SIGMA = 0.0\n",
    "\n",
    "prefix='DCSN_joint2'\n",
    "DEBUG=False\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train Convex-Optimization-Aware SR net')\n",
    "    \n",
    "    parser.add_argument('--SEED', type=int, default=1029)\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "\n",
    "    parser.add_argument('--resume_ind', type=int, default=999)\n",
    "    parser.add_argument('--snr', type=int, default=0)\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--workers', type=int, default=4)\n",
    "\n",
    "    ## Data generator configuration\n",
    "    parser.add_argument('--crop_size', type=int, default=256)\n",
    "    parser.add_argument('--scale', type=int, default=4)\n",
    "    parser.add_argument('--bands', type=int, default=172)\n",
    "    parser.add_argument('--msi_bands', type=int, default=4)\n",
    "    parser.add_argument('--mis_pix', type=int, default=0)\n",
    "    parser.add_argument('--mixed_align_opt', type=int, default=0)\n",
    "    parser.add_argument('--adaptive_fuse', type=int, default=3)\n",
    "    parser.add_argument('--student_layers', type=list, default=[1,2,2,1])\n",
    "    \n",
    "    # Network architecture configuration\n",
    "    parser.add_argument(\"--network_mode\", type=int, default=1, help=\"Training network mode: 0) Single mode, 1) LRHSI+HRMSI, 2) COCNN (LRHSI+HRMSI+CO), Default: 2\")     \n",
    "    parser.add_argument('--num_base_chs', type=int, default=86, help='The number of the channels of the base feature')\n",
    "    parser.add_argument('--num_blocks', type=int, default=6, help='The number of the repeated blocks in backbone')\n",
    "    parser.add_argument('--num_agg_feat', type=int, default=172//4, help='the additive feature maps in the block')\n",
    "    parser.add_argument('--groups', type=int, default=1, help=\"light version the group value can be >1, groups=1 for full COCNN version, groups=4 is COCNN-Light for 4 HRMSI version\")\n",
    "    \n",
    "    # Others\n",
    "    # parser.add_argument(\"--root\", type=str, default=\"/work/u1657859/DCSNv2/Data\", help='data root folder')\n",
    "    parser.add_argument(\"--root\", type=str, default=\"/work/u5088463/Data\", help='data root folder')   \n",
    "    parser.add_argument(\"--test_file\", type=str, default=\"./test.txt\")   \n",
    "    parser.add_argument(\"--prefix\", type=str, default=\"KD_QRCODE_band4\")  \n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\", help=\"cuda:device_id or cpu\")  \n",
    "    parser.add_argument(\"--DEBUG\", type=bool, default=False)  \n",
    "    parser.add_argument(\"--gpus\", type=int, default=1)\n",
    "    parser.add_argument(\"-f\", type=str, default=1)\n",
    "    # parser.add_argument(\"--test_mode\", type=int, default=2 , help='1 : test teacher , 2 : test student' ) \n",
    "    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.student_layers = list(map(int, args.student_layers))\n",
    "\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "m1=Student(args).cuda()\n",
    "# m2=Teacher(args)\n",
    "def prepare_input(resolution):\n",
    "    lr_hsi = torch.FloatTensor(1,172,64,64).cuda()\n",
    "    hr_msi = torch.FloatTensor(1,4,256,256).cuda()\n",
    "    return dict(LRHSI =lr_hsi, HRMSI=hr_msi)\n",
    "        \n",
    "\n",
    "##==========================================\n",
    "from ptflops import get_model_complexity_info\n",
    "device = 'cuda:0'\n",
    "# model = UNet(172,4).cuda()\n",
    "flops, params = get_model_complexity_info(m1, input_res=(1, 224, 224), \n",
    "                                          input_constructor=prepare_input,\n",
    "                                          as_strings=True, print_per_layer_stat=False)\n",
    "print('      - Flops:  ' + flops)\n",
    "print('      - Params: ' + params)\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "LRHSI = torch.FloatTensor(1,172,64,64).cuda()\n",
    "HRMSI = torch.FloatTensor(1,4,256,256).cuda()        \n",
    "flops, params = profile(m1, inputs=(LRHSI,HRMSI, ))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')\n",
    "\n",
    "# flops, params = profile(m2, inputs=(input2,input1, ))\n",
    "# print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "# print('Params = ' + str(params/1000**2) + 'M')\n",
    "\n",
    "\n",
    "repetitions = 1024\n",
    "# LRHSI = torch.FloatTensor(1,172,64,64).cuda()\n",
    "# HRMSI = torch.FloatTensor(1,4,256,256).cuda()\n",
    "    \n",
    "m1=m1.cuda()\n",
    "print('GPU warm up ...\\n')\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = m1(LRHSI, HRMSI)\n",
    "\n",
    "# synchronize 等待所有 GPU 任务处理完才返回 CPU 主线程\n",
    "torch.cuda.synchronize()\n",
    "# 设置用于测量时间的 cuda Event, 这是PyTorch 官方推荐的接口,理论上应该最靠谱\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "# 初始化一个时间容器\n",
    "timings = np.zeros((repetitions, 1))\n",
    "print('Inference Time Analysis ...\\n')\n",
    "with torch.no_grad():\n",
    "    for rep in tqdm.tqdm(range(repetitions)):\n",
    "        starter.record()\n",
    "        _ = m1(LRHSI, HRMSI)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize() # 等待GPU任务完成\n",
    "        curr_time = starter.elapsed_time(ender) # 从 starter 到 ender 之间用时,单位为毫秒\n",
    "        timings[rep] = curr_time\n",
    "avg = timings.sum()/repetitions\n",
    "print('\\nTotal={} ms'.format(timings.sum()))\n",
    "\n",
    "print('\\nAvg={} ms'.format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a48edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "      - Flops:  102.04 GMac\n",
      "      - Params: 7.33 M\n",
      "torch.Size([1, 172, 256, 256]) torch.Size([1, 172, 256, 256])\n",
      "GPU warm up ...\n",
      "\n",
      "Inference Time Analysis ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:38<00:00, 26.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total=36354.67979621887 ms\n",
      "\n",
      "Avg=35.50261698849499 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "from ptflops import get_model_complexity_info\n",
    "import tqdm\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = True\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes//4, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes//4, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes//2, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // 2, 1, 3, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(x)))\n",
    "        return self.sigmoid(avg_out)\n",
    "\n",
    "\n",
    "class mrcam(nn.Module):\n",
    "    def __init__(self, hs_channels,mschannels, bias=True):\n",
    "        super(mrcam, self).__init__()\n",
    "        self.conv0 = nn.Sequential(nn.Conv2d(hs_channels+mschannels, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "                     nn.LeakyReLU(0.2)\n",
    "         )\n",
    "\n",
    "        self.conv0_1 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.conv1_1 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "                nn.LeakyReLU(0.2),\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "        )\n",
    "        self.conv1_2 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "\n",
    "        self.conv3=nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "                nn.LeakyReLU(0.2),\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "                nn.LeakyReLU(0.2),\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "        )\n",
    "        self.conv1_3 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "\n",
    "        self.cb1 = nn.Conv2d(64*2, 64, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.cb2 = nn.Conv2d(64*3, 64, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        self.cb = nn.Sequential(\n",
    "            nn.Conv2d(64*3, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.channel_3_3 = ChannelAttention(64)\n",
    "        self.spatial_3_3 = SpatialAttention(64)\n",
    "        self.factor_3 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "        self.channel_5_5 = ChannelAttention(64)\n",
    "        self.spatial_5_5 = SpatialAttention(64)\n",
    "        self.factor_5 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "        self.channel_7_7 = ChannelAttention(64)\n",
    "        self.spatial_7_7 = SpatialAttention(64)\n",
    "        self.factor_7 = nn.Parameter(torch.FloatTensor([1]),requires_grad=True)\n",
    "        self.fusion = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(64*3, 64, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "\n",
    "                nn.LeakyReLU(0.2),\n",
    "\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x = self.conv0(x)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        cb1 = torch.cat((x1*self.conv1_1, x2*self.conv1_2), dim=1)\n",
    "        cb2 = torch.cat((x1*self.conv1_1, x2*self.conv1_2, x3*self.conv1_3), dim=1)\n",
    "\n",
    "        cb1 = self.cb1(cb1)\n",
    "        cb2 = self.cb2(cb2)\n",
    "        cb3 = torch.cat((x1*self.conv1_1,cb1,cb2),dim=1)\n",
    "        x = self.cb(cb3)\n",
    "\n",
    "        atten1 = self.channel_3_3(x)*self.spatial_3_3(x)*x + x\n",
    "        atten2 = self.channel_5_5(x)*self.spatial_5_5(x)*x + x\n",
    "        atten3 = self.channel_7_7(x)*self.spatial_7_7(x)*x + x\n",
    "        atten1 = self.factor_3*atten1\n",
    "        atten2 = self.factor_5*atten2\n",
    "        atten3 = self.factor_7*atten3\n",
    "\n",
    "        atten = torch.cat((atten1, atten2, atten3), dim=1)\n",
    "        x = self.fusion(atten) + x0\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, hschannels, mschannels, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.bilinear = bilinear\n",
    "        factor = 2 if bilinear else 1\n",
    "        #hsi 主网络\n",
    "        self.inc = DoubleConv(64, 64)\n",
    "        self.mscsam = mrcam(hschannels, mschannels)\n",
    "        self.down1 = Down(64+32, 128)\n",
    "        self.down2 = Down(128+64, 256)\n",
    "        self.down3 = Down(256+128, 512 // factor)\n",
    "        # self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(256+(512 // factor)+256// factor, 256 // factor, 256, bilinear)\n",
    "        self.up2 = Up(128+256 // factor+128// factor, 128 // factor, 128, bilinear)\n",
    "        self.up3 = Up(64+128 // factor+64// factor, 64, 64, bilinear)\n",
    "        # self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, hschannels)\n",
    "        #ms特征提取网络\n",
    "        self.incm = DoubleConv(mschannels, 32)\n",
    "        self.downm1 = Down(32, 64)\n",
    "        self.downm2 = Down(64, 128)\n",
    "        self.downm3 = Down(128, 256 // factor)\n",
    "        # self.down4 = Down(512, 1024 // factor)\n",
    "        # self.upm1 = Up(256, 128 // factor, bilinear)\n",
    "        # self.upm2 = Up(128, 64 // factor, bilinear)\n",
    "        # self.upm3 = Up(64, 32, bilinear)\n",
    "        self.upm1 = Upm(256 // factor, 128 // factor, bilinear)\n",
    "        self.upm2 = Upm(128 // factor, 64 // factor, bilinear)\n",
    "        self.upm3 = Upm(64 // factor, 64, bilinear)\n",
    "        # self.up4 = Up(128, 64, bilinear)\n",
    "        self.outmc = OutConv(64, hschannels)\n",
    "\n",
    "    def forward(self, zz):\n",
    "        y0=zz[1]\n",
    "        y1 = self.incm(zz[1])\n",
    "        y2 = self.downm1(y1)\n",
    "        y3 = self.downm2(y2)\n",
    "        y4 = self.downm3(y3)\n",
    "        # y5 = self.upm1(y4, y3)\n",
    "        # y6 = self.upm2(y5, y2)\n",
    "        # y = self.upm3(y6, y1)\n",
    "        y5 = self.upm1(y4)\n",
    "        y6 = self.upm2(y5)\n",
    "        y = self.upm3(y6)\n",
    "        logitsy = self.outmc(y)\n",
    "\n",
    "        x0 = zz[0]\n",
    "        # 嵌入多尺度注意力\n",
    "        # x1 = self.mscsam(x)\n",
    "\n",
    "        x1 = self.mscsam(torch.cat((zz[0],y0),dim=1))\n",
    "        x1 = self.inc(x1)\n",
    "\n",
    "        x2 = self.down1(torch.cat((x1, y1), dim=1))\n",
    "        x3 = self.down2(torch.cat((x2, y2), dim=1))\n",
    "        x4 = self.down3(torch.cat((x3, y3), dim=1))\n",
    "        # x5 = self.down4(x4)\n",
    "        x = self.up1(torch.cat((x4, y4),dim=1),x3)\n",
    "        x = self.up2(torch.cat((x, y5),dim=1),x2)\n",
    "        x = self.up3(torch.cat((x, y6),dim=1),x1)\n",
    "        # x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        # return logits+x0, logits+x0\n",
    "\n",
    "        return logits+x0, logits+x0+logitsy\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.Upsample(mode='bilinear',scale_factor=1/2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "            # nn.MaxPool2d(2),\n",
    "            # DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels-channels, (in_channels-channels) // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv((in_channels-channels) // 2+channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "class Upm(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels // 2, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x1 = self.up(x1)\n",
    "        return self.conv(x1)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3,stride=1,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "def prepare_input(resolution):\n",
    "    lr_hsi = torch.FloatTensor(1,172,256,256).cuda()\n",
    "    hr_msi = torch.FloatTensor(1,4,256,256).cuda()\n",
    "    return dict(zz = [lr_hsi,hr_msi])\n",
    "        \n",
    "\n",
    "##==========================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Flops & Parameter\n",
    "    # Create a network and a corresponding input\n",
    "    device = 'cuda:0'\n",
    "    model = UNet(172,4).cuda()\n",
    "    flops, params = get_model_complexity_info(model, input_res=(1, 224, 224), \n",
    "                                              input_constructor=prepare_input,\n",
    "                                              as_strings=True, print_per_layer_stat=False)\n",
    "    print('      - Flops:  ' + flops)\n",
    "    print('      - Params: ' + params)\n",
    "    \n",
    "    LRHSI = torch.FloatTensor(1,172,256,256).cuda()\n",
    "    HRMSI = torch.FloatTensor(1,4,256,256).cuda()\n",
    "    zz = [LRHSI,HRMSI]\n",
    "    y1,y2=model(zz)\n",
    "    print(y1.shape, y2.shape)\n",
    "  \n",
    "\n",
    "    #Use Profile\n",
    "    #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    #    with record_function(\"model_inference\"):\n",
    "    #        model(zz)\n",
    "    #print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "    #Use EndTime - StartTime with GPU synchronize\n",
    "    repetitions = 1024\n",
    "    print('GPU warm up ...\\n')\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(zz)\n",
    "    \n",
    "    # synchronize 等待所有 GPU 任务处理完才返回 CPU 主线程\n",
    "    torch.cuda.synchronize()\n",
    "    # 设置用于测量时间的 cuda Event, 这是PyTorch 官方推荐的接口,理论上应该最靠谱\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    # 初始化一个时间容器\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "    print('Inference Time Analysis ...\\n')\n",
    "    with torch.no_grad():\n",
    "        for rep in tqdm.tqdm(range(repetitions)):\n",
    "            starter.record()\n",
    "            _ = model(zz)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize() # 等待GPU任务完成\n",
    "            curr_time = starter.elapsed_time(ender) # 从 starter 到 ender 之间用时,单位为毫秒\n",
    "            timings[rep] = curr_time\n",
    "    avg = timings.sum()/repetitions\n",
    "    print('\\nTotal={} ms'.format(timings.sum()))\n",
    "    \n",
    "    print('\\nAvg={} ms'.format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09023a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
